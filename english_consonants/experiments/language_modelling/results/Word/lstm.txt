(english-consonants-venv):~/Experiments/EnglishConsonants$ python english_consonants/experiments/language_modelling/run_experiment.py --gpu_devices=0 --vocab_coverag
e=1 --batch_size=32 --model_type=rnn
Found cached dataset wikitext (.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1610.92it/s]
tokenizer class is: <class 'english_consonants.tokenizers.WordTokenizer'>
Loading cached processed dataset at .cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1fd
f7473581b0bfb.arrow
Loading cached processed dataset at .cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-fe8
2178e12600f83.arrow
Loading cached processed dataset at .cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-61e
ce7f799b57ae9.arrow
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 144886.71it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 64356.74it/s]
training on normal English
number of train vocabs: 33,277
number of train tokens: 2,051,910
number of train unique characters: 281
number of train all characters: 8,655,091
train words entropy: 10.241532170266325
train chars entropy: 4.789766164159704
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tacti
cal role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the
same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the " Nameless " , a penal military unit serving the nation of
Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent mult
iple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Va
lkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 120986.75it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 252151.31it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 3703146.44it/s]
Sequence Length: 216
Building DataLoaders
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 39852.32it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 34167.26it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 40711.06it/s]
Train DataLoader: 742
        Val DataLoader: 76
        Test DataLoader: 90
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 17.0 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 17.1 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.913   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 731, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 729, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-r1nsr5q6'
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.462867736816406     │
│          val_ppl          │       34995.046875        │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 0:00:03 • 0:00:00 23.34it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │ 17.0 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │ 17.1 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 25.5 M
Non-trainable params: 0
Total params: 25.5 M
Total estimated model params size (MB): 101
Epoch 00050: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00052: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00054: reducing learning rate of group 0 to 1.2500e-04.
Epoch 53/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 742/742 0:01:28 • 0:00:00 9.32it/s v_num: xl7p ppl: 80.423 loss: 4.387 val_ppl: 109.25 val_loss: 4.663
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    4.6009955406188965     │
│         test_ppl          │    106.01875305175781     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 23.01it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=47-val_loss=4.658-step=35
616-1.0.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=47-val_loss=4.658-step=356
16-1.0.ckpt
Experiments/english-consonants-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `test_dataloader
`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    3.6671125888824463     │     4.657750129699707     │     4.599318981170654     │
│         test_ppl          │     39.5320930480957      │    108.55327606201172     │    105.61222076416016     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 23.03it/s
[{'test_ppl/dataloader_idx_0': 39.5320930480957, 'test_loss/dataloader_idx_0': 3.6671125888824463}, {'test_ppl/dataloader_idx_1': 108.55327606201172, 'test_loss/dataloader_idx_1': 4.65775012
9699707}, {'test_ppl/dataloader_idx_2': 105.61222076416016, 'test_loss/dataloader_idx_2': 4.599318981170654}]
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 5115.86 seconds
training on consonants English
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 155717.95it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 85166.52it/s]
number of consonants train vocabs: 25,538
number of consonants train tokens: 2,012,005
number of consonants train unique characters: 271
number of consonants train all characters: 5,614,159
train consonants words entropy: 9.781745212611602
train consonants chars entropy: 4.655944792415041
Some of the Dataset Samples after deleting vowels:

= Vlkyr Chrncls  =

Snjō n Vlkyr 3 : <nk> Chrncls ( Jpns : 戦場のヴァルキュリア3 , lt . Vlkyr f th Bttlfld 3 ) , cmmnly rfrrd t s Vlkyr Chrncls  tsd Jpn , s  tctcl rl @-@ plyng vd gm dvlpd by Sg nd Md.Vsn fr th
 PlySttn Prtbl . Rlsd n Jnry 2011 n Jpn , t s th thrd gm n th Vlkyr srs . <nk> th sm fsn f tctcl nd rl @-@ tm gmply s ts prdcssrs , th stry rns prlll t th frst gm nd fllws th " Nmlss " ,  pn
l mltry nt srvng th ntn f Gll drng th Scnd rpn Wr wh prfrm scrt blck prtns nd r pttd gnst th mprl nt " <nk> Rvn " .
Th gm bgn dvlpmnt n 2010 , crryng vr  lrg prtn f th wrk dn n Vlkyr Chrncls  . Whl t rtnd th stndrd ftrs f th srs , t ls ndrwnt mltpl djstmnts , sch s mkng th gm mr <nk> fr srs nwcmrs . Chrct
r dsgnr <nk> Hnj nd cmpsr Htsh Skmt bth rtrnd frm prvs ntrs , lng wth Vlkyr Chrncls  drctr Tksh zw .  lrg tm f wrtrs hndld th scrpt . Th gm 's pnng thm ws sng by My 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 127554.15it/s]
Considered Vocab (from WordTokenizer): 25,542
        All Vocab (WordTokenizer): 25,542
Training WordTokenizer ...
Tokenizer Vocab Size: 25,542
Calculating Sequence Length:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 232421.53it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 3767373.33it/s]
Sequence Length: 211
Building DataLoaders
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:01<00:00, 27532.46it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 41102.19it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 34891.82it/s]
Train DataLoader: 742
        Val DataLoader: 76
        Test DataLoader: 90
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 13.1 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 13.1 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
21.5 M    Trainable params
0         Non-trainable params
21.5 M    Total params
86.032    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.166621208190918     │
│          val_ppl          │      26022.69921875       │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 0:00:02 • 0:00:00 28.24it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │ 13.1 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │ 13.1 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 21.5 M
Non-trainable params: 0
Total params: 21.5 M
Total estimated model params size (MB): 86
Epoch 00048: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00051: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00053: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00055: reducing learning rate of group 0 to 6.2500e-05.
Epoch 55/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 742/742 0:01:12 • 0:00:00 11.44it/s v_num: 5num ppl: 68.443 loss: 4.226 val_ppl: 113.074 val_loss: 4.703
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    4.6330156326293945     │
│         test_ppl          │    109.06966400146484     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 28.38it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=48-val_loss=4.698-
step=36358-1.0.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=48-val_loss=4.698-s
tep=36358-1.0.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    3.8038830757141113     │     4.698048114776611     │     4.628124713897705     │
│         test_ppl          │    45.320159912109375     │    112.48771667480469     │    108.44271087646484     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 28.41it/s
[{'test_ppl/dataloader_idx_0': 45.320159912109375, 'test_loss/dataloader_idx_0': 3.8038830757141113}, {'test_ppl/dataloader_idx_1': 112.48771667480469, 'test_loss/dataloader_idx_1': 4.698048
114776611}, {'test_ppl/dataloader_idx_2': 108.44271087646484, 'test_loss/dataloader_idx_2': 4.628124713897705}]
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4429.13 seconds
training on masked consonants English
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 144316.20it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 59950.05it/s]
number of masked consonants train vocabs: 30,090
number of masked consonants train tokens: 2,051,910
number of masked consonants train unique characters: 272
number of masked consonants train all characters: 8,655,091
train masked consonants words entropy: 10.021776230734403
train masked consonants chars entropy: 3.955361446551099
Some of the Dataset Samples after masking:

= Valkyraa Chranaclas aaa =

Sanjō na Valkyraa 3 : <ank> Chranaclas ( Japanasa : 戦場のヴァルキュリア3 , lat . Valkyraa af tha Battlafaald 3 ) , cammanly rafarrad ta as Valkyraa Chranaclas aaa aatsada Japan , as a tacta
cal rala @-@ playang vadaa gama davalapad by Saga and Madaa.Vasaan far tha PlayStataan Partabla . Ralaasad an Janaary 2011 an Japan , at as tha thard gama an tha Valkyraa saraas . <ank> tha
sama fasaan af tactacal and raal @-@ tama gamaplay as ats pradacassars , tha stary rans parallal ta tha farst gama and fallaws tha " Namalass " , a panal malatary anat sarvang tha nataan af
Gallaa darang tha Sacand aarapan War wha parfarm sacrat black aparataans and ara pattad agaanst tha amparaal anat " <ank> Ravan " .
Tha gama bagan davalapmant an 2010 , carryang avar a larga partaan af tha wark dana an Valkyraa Chranaclas aa . Whala at rataanad tha standard faataras af tha saraas , at alsa andarwant malt
apla adjastmants , sach as makang tha gama mara <ank> far saraas nawcamars . Charactar dasagnar <ank> Hanjaa and campasar Hatasha Sakamata bath ratarnad fram pravaaas antraas , alang wath Va
lkyraa Chranaclas aa daractar Takasha azawa . a larga taam af wratars handlad tha scrapt . Tha gama 's apanang thama was sang by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 116974.09it/s]
Considered Vocab (from WordTokenizer): 30,094
        All Vocab (WordTokenizer): 30,094
Training WordTokenizer ...
Tokenizer Vocab Size: 30,094
Calculating Sequence Length:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 245154.35it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 3476600.62it/s]
Sequence Length: 216
Building DataLoaders
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 37429.97it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 39630.95it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 33308.57it/s]
Train DataLoader: 742
        Val DataLoader: 76
        Test DataLoader: 90
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 15.4 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 15.4 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
23.8 M    Trainable params
0         Non-trainable params
23.8 M    Total params
95.373    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.343059539794922     │
│          val_ppl          │      31044.181640625      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 0:00:03 • 0:00:00 24.82it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │ 15.4 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │ 15.4 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 23.8 M
Non-trainable params: 0
Total params: 23.8 M
Total estimated model params size (MB): 95
Epoch 00041: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00047: reducing learning rate of group 0 to 2.5000e-04.
Epoch 48/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 742/742 0:01:22 • 0:00:00 10.03it/s v_num: 4kxb ppl: 78.532 loss: 4.364 val_ppl: 106.081 val_loss: 4.637
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     4.579355716705322     │
│         test_ppl          │    103.59971618652344     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 24.72it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=48-val_loss
=4.637-step=36358-1.0.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/WordTokenizer/RNN/checkpoints/epoch=48-val_loss=
4.637-step=36358-1.0.ckpt
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 731, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 729, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-ibscwqjm'
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     3.684267044067383     │     4.636537551879883     │     4.579355716705322     │
│         test_ppl          │     40.25412368774414     │    106.08110046386719     │    103.59971618652344     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 0:00:03 • 0:00:00 24.70it/s
[{'test_ppl/dataloader_idx_0': 40.25412368774414, 'test_loss/dataloader_idx_0': 3.684267044067383}, {'test_ppl/dataloader_idx_1': 106.08110046386719, 'test_loss/dataloader_idx_1': 4.63653755
1879883}, {'test_ppl/dataloader_idx_2': 103.59971618652344, 'test_loss/dataloader_idx_2': 4.579355716705322}]
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4376.83 seconds

