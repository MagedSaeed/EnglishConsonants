(dotless-arabic) majed_alshaibani@jrcai14:~$ deactivate
majed_alshaibani@jrcai14:~$ cd Experiments/EnglishConsonants/
majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$ source ../english-consonants-venv/bin/activate
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$ python english_consonants/experiments/language_modelling/run_experiment.py --gpu_devices=0 --vocab_coverag
e=0.95 --batch_size=64 --tokenizer_class=sentencepiecetokenizer --model_type=rnn
using the raw version of the dataset for sentencepiece tokenizer
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1575.03it/s]
tokenizer class is: <class 'english_consonants.tokenizers.SentencePieceTokenizer'>
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-80473819125baabc.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-8b4403f6333217dd.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-8315a265a9718b54.arrow
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 140317.37it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 64000.10it/s]
training on normal English
number of train vocabs: 76,616
number of train tokens: 2,051,910
number of train unique characters: 1,011
number of train all characters: 8,793,497
train words entropy: 10.649483093472629
train chars entropy: 4.741493916170156
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a
tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Emplo
ying the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the " Nameless " , a penal military unit serving the n
ation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " Calamaty Raven " .
The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent mult
iple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along wit
h Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 117657.74it/s]
Considered Vocab (from WordTokenizer): 20,522
        All Vocab (WordTokenizer): 76,620
Training SentencePiece ...
Tokenizer Vocab Size: 20,522
Calculating Sequence Length:
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:145: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:04<00:00, 8299.99it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 3947769.97it/s]
Sequence Length: 245
Building DataLoaders
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:152: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:06<00:00, 5324.41it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 5140.77it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 5296.80it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 10.5 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 10.5 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
18.9 M    Trainable params
0         Non-trainable params
18.9 M    Total params
75.731    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │     9.733256340026855     │
│          val_ppl          │      16878.443359375      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 15.82it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │ 10.5 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │ 10.5 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 18.9 M
Non-trainable params: 0
Total params: 18.9 M
Total estimated model params size (MB): 75
Epoch 00054: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00059: reducing learning rate of group 0 to 2.5000e-04.
Epoch 60/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:13 • 0:00:01 6.13it/s v_num: 9n36 ppl: 7.315 loss: 1.99 val_ppl: 8.165 val_loss: 2.024
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.9600098133087158     │
│         test_ppl          │     8.331690788269043     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:03 • 0:00:00 14.98it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=60-val_loss=2.02
4-step=22630-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=60-val_loss=2.024
-step=22630-0.95.ckpt
/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `test_dataloader
`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.5426453351974487     │    2.0238230228424072     │    1.9600098133087158     │
│         test_ppl          │    4.7445173263549805     │     8.16488265991211      │     8.331690788269043     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 15.12it/s
[{'test_ppl/dataloader_idx_0': 4.7445173263549805, 'test_loss/dataloader_idx_0': 1.5426453351974487}, {'test_ppl/dataloader_idx_1': 8.16488265991211, 'test_loss/dataloader_idx_1': 2.02382302
28424072}, {'test_ppl/dataloader_idx_2': 8.331690788269043, 'test_loss/dataloader_idx_2': 1.9600098133087158}]
Training OOVs rate: 61.43
        Validation OOVs rate: 61.43
        Test OOVs rate: 61.43
Training Time: 4499.56 seconds
training on consonants English
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 140619.99it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 83300.76it/s]
number of consonants train vocabs: 56,195
number of consonants train tokens: 2,011,985
number of consonants train unique characters: 999
number of consonants train all characters: 5,654,984
train consonants words entropy: 10.151310973171505
train consonants chars entropy: 4.603438250163171
Some of the Dataset Samples after deleting vowels:

= Vlkyr Chrncls  =

Snjō n Vlkyr 3 : nrcrdd Chrncls ( Jpns : 戦場のヴァルキュリア3 , lt . Vlkyr f th Bttlfld 3 ) , cmmnly rfrrd t s Vlkyr Chrncls  tsd Jpn , s  tctcl rl @-@ plyng vd gm dvlpd by Sg nd Md.Vsn fr
th PlySttn Prtbl . Rlsd n Jnry 2011 n Jpn , t s th thrd gm n th Vlkyr srs . mplyng th sm fsn f tctcl nd rl @-@ tm gmply s ts prdcssrs , th stry rns prlll t th frst gm nd fllws th " Nmlss " ,
  pnl mltry nt srvng th ntn f Gll drng th Scnd rpn Wr wh prfrm scrt blck prtns nd r pttd gnst th mprl nt " Clmty Rvn " .
Th gm bgn dvlpmnt n 2010 , crryng vr  lrg prtn f th wrk dn n Vlkyr Chrncls  . Whl t rtnd th stndrd ftrs f th srs , t ls ndrwnt mltpl djstmnts , sch s mkng th gm mr frgvng fr srs nwcmrs . Chr
ctr dsgnr Rt Hnj nd cmpsr Htsh Skmt bth rtrnd frm prvs ntrs , lng wth Vlkyr Chrncls  drctr Tksh zw .  lrg tm f wrtrs hndld th scrpt . Th gm 's pnng thm ws sng by My 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 128539.77it/s]
Considered Vocab (from WordTokenizer): 13,084
        All Vocab (WordTokenizer): 56,199
Training SentencePiece ...
Tokenizer Vocab Size: 13,084
Calculating Sequence Length:
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:145: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:03<00:00, 11945.11it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 4001310.88it/s]
Sequence Length: 230
Building DataLoaders
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:152: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:05<00:00, 6446.49it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 6605.21it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 6604.55it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 6.7 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 6.7 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
15.1 M    Trainable params
0         Non-trainable params
15.1 M    Total params
60.468    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │     9.59435749053955      │
│          val_ppl          │     14682.3623046875      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:01 • 0:00:00 21.75it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │  6.7 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │  6.7 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 15.1 M
Non-trainable params: 0
Total params: 15.1 M
Total estimated model params size (MB): 60
Epoch 48/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:00:54 • 0:00:01 7.41it/s v_num: 6use ppl: 5.86 loss: 1.768 val_ppl: 8.153 val_loss: 2.018
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     1.937488079071045     │
│         test_ppl          │     8.200067520141602     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 21.26it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=48-val_lo
ss=2.018-step=18178-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=48-val_los
s=2.018-step=18178-0.95.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.6402913331985474     │     2.018122911453247     │     1.937488079071045     │
│         test_ppl          │     5.248965740203857     │     8.152679443359375     │     8.200067520141602     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 21.35it/s
[{'test_ppl/dataloader_idx_0': 5.248965740203857, 'test_loss/dataloader_idx_0': 1.6402913331985474}, {'test_ppl/dataloader_idx_1': 8.152679443359375, 'test_loss/dataloader_idx_1': 2.01812291
1453247}, {'test_ppl/dataloader_idx_2': 8.200067520141602, 'test_loss/dataloader_idx_2': 1.937488079071045}]
Training OOVs rate: 61.32
        Validation OOVs rate: 61.32
        Test OOVs rate: 61.32
Training Time: 2662.10 seconds
training on masked consonants English
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████| 36718/36718 [00:00<00:00, 134230.77it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████| 36718/36718 [00:00<00:00, 59654.60it/s]
number of masked consonants train vocabs: 67,965
number of masked consonants train tokens: 2,051,910
number of masked consonants train unique characters: 1,000
number of masked consonants train all characters: 8,793,497
train masked consonants words entropy: 10.414059778904265
train masked consonants chars entropy: 3.9005023370188336
Some of the Dataset Samples after masking:

= Valkyraa Chranaclas aaa =

Sanjō na Valkyraa 3 : anracardad Chranaclas ( Japanasa : 戦場のヴァルキュリア3 , lat . Valkyraa af tha Battlafaald 3 ) , cammanly rafarrad ta as Valkyraa Chranaclas aaa aatsada Japan , as a
tactacal rala @-@ playang vadaa gama davalapad by Saga and Madaa.Vasaan far tha PlayStataan Partabla . Ralaasad an Janaary 2011 an Japan , at as tha thard gama an tha Valkyraa saraas . ampla
yang tha sama fasaan af tactacal and raal @-@ tama gamaplay as ats pradacassars , tha stary rans parallal ta tha farst gama and fallaws tha " Namalass " , a panal malatary anat sarvang tha n
ataan af Gallaa darang tha Sacand aarapan War wha parfarm sacrat black aparataans and ara pattad agaanst tha amparaal anat " Calamaty Ravan " .
Tha gama bagan davalapmant an 2010 , carryang avar a larga partaan af tha wark dana an Valkyraa Chranaclas aa . Whala at rataanad tha standard faataras af tha saraas , at alsa andarwant malt
apla adjastmants , sach as makang tha gama mara fargavang far saraas nawcamars . Charactar dasagnar Raata Hanjaa and campasar Hatasha Sakamata bath ratarnad fram pravaaas antraas , alang wat
h Valkyraa Chranaclas aa daractar Takasha azawa . a larga taam af wratars handlad tha scrapt . Tha gama 's apanang thama was sang by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████| 36718/36718 [00:00<00:00, 110171.68it/s]
Considered Vocab (from WordTokenizer): 17,247
        All Vocab (WordTokenizer): 67,969
Training SentencePiece ...
Tokenizer Vocab Size: 17,247
Calculating Sequence Length:
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████| 36718/36718 [00:04<00:00, 8162.51it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████| 36718/36718 [00:00<00:00, 3907999.75it/s]
Sequence Length: 239
Building DataLoaders
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████| 36718/36718 [00:06<00:00, 5292.36it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████| 3760/3760 [00:00<00:00, 5090.45it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████| 4358/4358 [00:00<00:00, 5259.16it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type       | Params
-----------------------------------------------
0 | embedding_layer | Embedding  | 8.8 M
1 | rnn             | LSTM       | 8.4 M
2 | dropout_layer   | Dropout    | 0
3 | relu            | ReLU       | 0
4 | dense_layer     | Linear     | 8.8 M
5 | train_ppl       | Perplexity | 0
6 | val_ppl         | Perplexity | 0
7 | test_ppl        | Perplexity | 0
-----------------------------------------------
17.3 M    Trainable params
0         Non-trainable params
17.3 M    Total params
69.011    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.022161483764648     │
│          val_ppl          │      22532.740234375      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 17.81it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type       ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding  │  8.8 M │
│ 1 │ rnn             │ LSTM       │  8.4 M │
│ 2 │ dropout_layer   │ Dropout    │      0 │
│ 3 │ relu            │ ReLU       │      0 │
│ 4 │ dense_layer     │ Linear     │  8.8 M │
│ 5 │ train_ppl       │ Perplexity │      0 │
│ 6 │ val_ppl         │ Perplexity │      0 │
│ 7 │ test_ppl        │ Perplexity │      0 │
└───┴─────────────────┴────────────┴────────┘
Trainable params: 17.3 M

Non-trainable params: 0

Total params: 17.3 M

Total estimated model params size (MB): 69

Epoch 50/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:04 • 0:00:01 7.11it/s v_num: 5utf ppl: 7.251 loss: 1.981 val_ppl: 8.069 val_loss: 2.01
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.9396382570266724     │
│         test_ppl          │      8.1722993850708      │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 17.43it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=50
-val_loss=2.010-step=18920-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/SentencePieceTokenizer/RNN/checkpoints/epoch=50-
val_loss=2.010-step=18920-0.95.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.6016571521759033     │     2.009995460510254     │    1.9396382570266724     │
│         test_ppl          │     5.036617279052734     │     8.069159507751465     │      8.1722993850708      │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 17.42it/s
[{'test_ppl/dataloader_idx_0': 5.036617279052734, 'test_loss/dataloader_idx_0': 1.6016571521759033}, {'test_ppl/dataloader_idx_1': 8.069159507751465, 'test_loss/dataloader_idx_1': 2.00999546
0510254}, {'test_ppl/dataloader_idx_2': 8.1722993850708, 'test_loss/dataloader_idx_2': 1.9396382570266724}]
Training OOVs rate: 61.41
        Validation OOVs rate: 61.41
        Test OOVs rate: 61.41
Training Time: 3307.48 seconds
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$
