(dotless-arabic) majed_alshaibani@jrcai14:~$ deactivate
majed_alshaibani@jrcai14:~$ cd Experiments/EnglishConsonants
majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$ source ../english-consonants-venv/bin/activate
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$ python english_consonants/experiments/language_modelling/run_experiment.py --gpu_devices=1 --vocab_coverag
e=0.95 --batch_size=64 --tokenizer_class=sentencepiecetokenizer
using the raw version of the dataset for sentencepiece tokenizer
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1577.40it/s]
tokenizer class is: <class 'english_consonants.tokenizers.SentencePieceTokenizer'>
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-80473819125baabc.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-8b4403f6333217dd.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache
-8315a265a9718b54.arrow
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 136982.90it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 63665.21it/s]
training on normal English
number of train vocabs: 76,616
number of train tokens: 2,051,910
number of train unique characters: 1,011
number of train all characters: 8,793,497
train words entropy: 10.649483093472629
train chars entropy: 4.741493916170156
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a
tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Emplo
ying the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the " Nameless " , a penal military unit serving the n
ation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit " Calamaty Raven " .
The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent mult
iple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along wit
h Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 116748.42it/s]
Considered Vocab (from WordTokenizer): 20,522
        All Vocab (WordTokenizer): 76,620
Training SentencePiece ...
Tokenizer Vocab Size: 20,522
Calculating Sequence Length:
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:145: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:04<00:00, 8230.82it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 4052908.08it/s]
Sequence Length: 245
Building DataLoaders
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:152: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:07<00:00, 5117.65it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 5106.20it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 5264.99it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name                | Type               | Params
-----------------------------------------------------------
0 | pos_encoder         | PositionalEncoding | 0
1 | embedding           | Embedding          | 4.1 M
2 | transformer_encoder | TransformerEncoder | 484 K
3 | linear              | Linear             | 4.1 M
4 | train_ppl           | Perplexity         | 0
5 | val_ppl             | Perplexity         | 0
6 | test_ppl            | Perplexity         | 0
-----------------------------------------------------------
8.7 M     Trainable params
0         Non-trainable params
8.7 M     Total params
34.853    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 731, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 729, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-u6evsijg'
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │     9.80749225616455      │
│          val_ppl          │      18210.45703125       │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:04 • 0:00:00 13.17it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                ┃ Type               ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ pos_encoder         │ PositionalEncoding │      0 │
│ 1 │ embedding           │ Embedding          │  4.1 M │
│ 2 │ transformer_encoder │ TransformerEncoder │  484 K │
│ 3 │ linear              │ Linear             │  4.1 M │
│ 4 │ train_ppl           │ Perplexity         │      0 │
│ 5 │ val_ppl             │ Perplexity         │      0 │
│ 6 │ test_ppl            │ Perplexity         │      0 │
└───┴─────────────────────┴────────────────────┴────────┘
Trainable params: 8.7 M
Non-trainable params: 0
Total params: 8.7 M
Total estimated model params size (MB): 34
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 731, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 729, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-lbp1nio5'
Epoch 00025: reducing learning rate of group 0 to 1.2500e+00.
Epoch 40/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:17 • 0:00:01 5.69it/s v_num: bojv ppl: 3.585 loss: 1.277 val_ppl: 7.458 val_loss: 1.929
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.8514848947525024     │
│         test_ppl          │     7.548233509063721     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:03 • 0:00:00 13.00it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/epoch=39-val_l
oss=1.926-step=14839-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/all-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/epoch=39-val_lo
ss=1.926-step=14839-0.95.ckpt
/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `test_dataloader
`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     1.340449333190918     │    1.9257363080978394     │    1.8482987880706787     │
│         test_ppl          │    3.8732521533966064     │     7.432727813720703     │     7.515402793884277     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:03 • 0:00:00 13.09it/s
[{'test_ppl/dataloader_idx_0': 3.8732521533966064, 'test_loss/dataloader_idx_0': 1.340449333190918}, {'test_ppl/dataloader_idx_1': 7.432727813720703, 'test_loss/dataloader_idx_1': 1.92573630
80978394}, {'test_ppl/dataloader_idx_2': 7.515402793884277, 'test_loss/dataloader_idx_2': 1.8482987880706787}]
Training OOVs rate: 61.43
        Validation OOVs rate: 61.43
        Test OOVs rate: 61.43
Training Time: 3163.88 seconds
training on consonants English
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 141950.07it/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 84053.19it/s]
number of consonants train vocabs: 56,195
number of consonants train tokens: 2,011,985
number of consonants train unique characters: 999
number of consonants train all characters: 5,654,984
train consonants words entropy: 10.151310973171505
train consonants chars entropy: 4.603438250163171
Some of the Dataset Samples after deleting vowels:

= Vlkyr Chrncls  =

Snjō n Vlkyr 3 : nrcrdd Chrncls ( Jpns : 戦場のヴァルキュリア3 , lt . Vlkyr f th Bttlfld 3 ) , cmmnly rfrrd t s Vlkyr Chrncls  tsd Jpn , s  tctcl rl @-@ plyng vd gm dvlpd by Sg nd Md.Vsn fr
th PlySttn Prtbl . Rlsd n Jnry 2011 n Jpn , t s th thrd gm n th Vlkyr srs . mplyng th sm fsn f tctcl nd rl @-@ tm gmply s ts prdcssrs , th stry rns prlll t th frst gm nd fllws th " Nmlss " ,
  pnl mltry nt srvng th ntn f Gll drng th Scnd rpn Wr wh prfrm scrt blck prtns nd r pttd gnst th mprl nt " Clmty Rvn " .
Th gm bgn dvlpmnt n 2010 , crryng vr  lrg prtn f th wrk dn n Vlkyr Chrncls  . Whl t rtnd th stndrd ftrs f th srs , t ls ndrwnt mltpl djstmnts , sch s mkng th gm mr frgvng fr srs nwcmrs . Chr
ctr dsgnr Rt Hnj nd cmpsr Htsh Skmt bth rtrnd frm prvs ntrs , lng wth Vlkyr Chrncls  drctr Tksh zw .  lrg tm f wrtrs hndld th scrpt . Th gm 's pnng thm ws sng by My 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 128964.08it/s]
Considered Vocab (from WordTokenizer): 13,084
        All Vocab (WordTokenizer): 56,199
Training SentencePiece ...
Tokenizer Vocab Size: 13,084
Calculating Sequence Length:
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:145: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:03<00:00, 11986.83it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:00<00:00, 4159863.17it/s]
Sequence Length: 230
Building DataLoaders
  0%|                                                                                                                                                               | 0/36718 [00:00<?, ?it/s]
/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonants/tokenizers.py:152: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode
  warnings.warn("sentencepiece tokenizer cannot split text unless with PBE mode")
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36718/36718 [00:05<00:00, 6517.20it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 6569.18it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 6565.75it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name                | Type               | Params
-----------------------------------------------------------
0 | pos_encoder         | PositionalEncoding | 0
1 | embedding           | Embedding          | 2.6 M
2 | transformer_encoder | TransformerEncoder | 484 K
3 | linear              | Linear             | 2.6 M
4 | train_ppl           | Perplexity         | 0
5 | val_ppl             | Perplexity         | 0
6 | test_ppl            | Perplexity         | 0
-----------------------------------------------------------
5.7 M     Trainable params
0         Non-trainable params
5.7 M     Total params
22.923    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │     9.834502220153809     │
│          val_ppl          │      18669.216796875      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 19.03it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                ┃ Type               ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ pos_encoder         │ PositionalEncoding │      0 │
│ 1 │ embedding           │ Embedding          │  2.6 M │
│ 2 │ transformer_encoder │ TransformerEncoder │  484 K │
│ 3 │ linear              │ Linear             │  2.6 M │
│ 4 │ train_ppl           │ Perplexity         │      0 │
│ 5 │ val_ppl             │ Perplexity         │      0 │
│ 6 │ test_ppl            │ Perplexity         │      0 │
└───┴─────────────────────┴────────────────────┴────────┘
Trainable params: 5.7 M
Non-trainable params: 0
Total params: 5.7 M
Total estimated model params size (MB): 22
Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finalizers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 731, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 729, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-2vgymbt4'
Epoch 00025: reducing learning rate of group 0 to 1.2500e+00.
Epoch 00029: reducing learning rate of group 0 to 3.1250e-01.
Epoch 00035: reducing learning rate of group 0 to 7.8125e-02.
Epoch 00041: reducing learning rate of group 0 to 1.9531e-02.
Epoch 42/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:00:56 • 0:00:01 6.88it/s v_num: 4ljk ppl: 4.297 loss: 1.458 val_ppl: 7.541 val_loss: 1.936
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.8385603427886963     │
│         test_ppl          │    7.4979023933410645     │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 19.04it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/epoch=3
8-val_loss=1.935-step=14468-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/consonants-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/epoch=38
-val_loss=1.935-step=14468-0.95.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │    1.4748786687850952     │     1.935402274131775     │    1.8383468389511108     │
│         test_ppl          │    4.4385151863098145     │     7.538851261138916     │    7.4958930015563965     │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 19.06it/s
[{'test_ppl/dataloader_idx_0': 4.4385151863098145, 'test_loss/dataloader_idx_0': 1.4748786687850952}, {'test_ppl/dataloader_idx_1': 7.538851261138916, 'test_loss/dataloader_idx_1': 1.9354022
74131775}, {'test_ppl/dataloader_idx_2': 7.4958930015563965, 'test_loss/dataloader_idx_2': 1.8383468389511108}]
Training OOVs rate: 61.32
        Validation OOVs rate: 61.32
        Test OOVs rate: 61.32
Training Time: 2424.39 seconds
training on masked consonants English
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████| 36718/36718 [00:00<00:00, 137161.39it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████| 36718/36718 [00:00<00:00, 62627.29it/s]
number of masked consonants train vocabs: 67,965
number of masked consonants train tokens: 2,051,910
number of masked consonants train unique characters: 1,000
number of masked consonants train all characters: 8,793,497
train masked consonants words entropy: 10.414059778904265
train masked consonants chars entropy: 3.9005023370188336
Some of the Dataset Samples after masking:

= Valkyraa Chranaclas aaa =

Sanjō na Valkyraa 3 : anracardad Chranaclas ( Japanasa : 戦場のヴァルキュリア3 , lat . Valkyraa af tha Battlafaald 3 ) , cammanly rafarrad ta as Valkyraa Chranaclas aaa aatsada Japan , as a
tactacal rala @-@ playang vadaa gama davalapad by Saga and Madaa.Vasaan far tha PlayStataan Partabla . Ralaasad an Janaary 2011 an Japan , at as tha thard gama an tha Valkyraa saraas . ampla
yang tha sama fasaan af tactacal and raal @-@ tama gamaplay as ats pradacassars , tha stary rans parallal ta tha farst gama and fallaws tha " Namalass " , a panal malatary anat sarvang tha n
ataan af Gallaa darang tha Sacand aarapan War wha parfarm sacrat black aparataans and ara pattad agaanst tha amparaal anat " Calamaty Ravan " .
Tha gama bagan davalapmant an 2010 , carryang avar a larga partaan af tha wark dana an Valkyraa Chranaclas aa . Whala at rataanad tha standard faataras af tha saraas , at alsa andarwant malt
apla adjastmants , sach as makang tha gama mara fargavang far saraas nawcamars . Charactar dasagnar Raata Hanjaa and campasar Hatasha Sakamata bath ratarnad fram pravaaas antraas , alang wat
h Valkyraa Chranaclas aa daractar Takasha azawa . a larga taam af wratars handlad tha scrapt . Tha gama 's apanang thama was sang by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████| 36718/36718 [00:00<00:00, 114534.02it/s]
Considered Vocab (from WordTokenizer): 17,247
        All Vocab (WordTokenizer): 67,969
Training SentencePiece ...
Tokenizer Vocab Size: 17,247
Calculating Sequence Length:
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████| 36718/36718 [00:04<00:00, 7513.47it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████| 36718/36718 [00:00<00:00, 4056644.57it/s]
Sequence Length: 239
Building DataLoaders
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████| 36718/36718 [00:07<00:00, 5172.49it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████| 3760/3760 [00:00<00:00, 5202.26it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████| 4358/4358 [00:00<00:00, 5297.95it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name                | Type               | Params
-----------------------------------------------------------
0 | pos_encoder         | PositionalEncoding | 0
1 | embedding           | Embedding          | 3.4 M
2 | transformer_encoder | TransformerEncoder | 484 K
3 | linear              | Linear             | 3.5 M
4 | train_ppl           | Perplexity         | 0
5 | val_ppl             | Perplexity         | 0
6 | test_ppl            | Perplexity         | 0
-----------------------------------------------------------
7.4 M     Trainable params
0         Non-trainable params
7.4 M     Total params
29.600    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.016278266906738     │
│          val_ppl          │      22394.369140625      │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 15.10it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name                ┃ Type               ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ pos_encoder         │ PositionalEncoding │      0 │
│ 1 │ embedding           │ Embedding          │  3.4 M │
│ 2 │ transformer_encoder │ TransformerEncoder │  484 K │
│ 3 │ linear              │ Linear             │  3.5 M │
│ 4 │ train_ppl           │ Perplexity         │      0 │
│ 5 │ val_ppl             │ Perplexity         │      0 │
│ 6 │ test_ppl            │ Perplexity         │      0 │
└───┴─────────────────────┴────────────────────┴────────┘
Trainable params: 7.4 M

Non-trainable params: 0

Total params: 7.4 M

Total estimated model params size (MB): 29

Epoch 00024: reducing learning rate of group 0 to 1.2500e+00.
Epoch 38/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:09 • 0:00:01 6.35it/s v_num: dz0f ppl: 3.658 loss: 1.297 val_ppl: 7.425 val_loss: 1.924
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     1.843883991241455     │
│         test_ppl          │     7.51849365234375      │
└───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 15.07it/s
test results for dataloaders train,val,test as index 0,1, and 2 accordingly
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off
precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Restoring states from the checkpoint path at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/
epoch=38-val_loss=1.924-step=14468-0.95.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from the checkpoint at /home/majed_alshaibani/Experiments/EnglishConsonants/EnglishNLMs/masked-consonants-wikitext-chars/SentencePieceTokenizer/Transformer/checkpoints/e
poch=38-val_loss=1.924-step=14468-0.95.ckpt
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃       DataLoader 1        ┃       DataLoader 2        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         test_loss         │     1.388364553451538     │    1.9242210388183594     │     1.843883991241455     │
│         test_ppl          │     4.071954250335693     │     7.424582004547119     │     7.51849365234375      │
└───────────────────────────┴───────────────────────────┴───────────────────────────┴───────────────────────────┘
Testing ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 0:00:02 • 0:00:00 15.13it/s
[{'test_ppl/dataloader_idx_0': 4.071954250335693, 'test_loss/dataloader_idx_0': 1.388364553451538}, {'test_ppl/dataloader_idx_1': 7.424582004547119, 'test_loss/dataloader_idx_1': 1.924221038
8183594}, {'test_ppl/dataloader_idx_2': 7.51849365234375, 'test_loss/dataloader_idx_2': 1.843883991241455}]
Training OOVs rate: 61.41
        Validation OOVs rate: 61.41
        Test OOVs rate: 61.41
Training Time: 2705.96 seconds
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsonants$
