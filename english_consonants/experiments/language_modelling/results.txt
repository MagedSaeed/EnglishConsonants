86.968    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │     10.64254379272461     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 16.
05it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 15.4 M │
│ 1 │ gru_layer          │ GRU       │  6.3 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 15.4 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 21.7 M


Non-trainable params: 0


Total params: 21.7 M


Total estimated model params size (MB): 86


Epoch 00032: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00034: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00036: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00038: reducing learning rate of group 0 to 6.2500e-05.
Epoch 39/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:12 • 0:00:01
6.11it/s v_num: k2gk loss: 4.173 val_loss: 4.958
100%|██████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████████████| 36718/3
6718 [00:01<00:00, 33289.31it/s]
100%|██████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████████████████████████|
371/371 [00:27<00:00, 13.28it/s]
100%|██████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████████████| 4358/
4358 [00:00<00:00, 33058.69it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
| 45/45 [00:06<00:00,  6.95it/s]
100%|██████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████████████| 4358/
4358 [00:00<00:00, 38221.81it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
| 45/45 [00:06<00:00,  6.96it/s]
Training Perplexity: 53.66225311163192
        Perplexity with OOVs: 136.08851656856154
        Perplexity without OOVs: 136.08851656856154
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 2897.50 seconds
predicting: =
prompt is: <bos> =
predicting: a
prompt is: <bos> = a
predicting: <ank>
prompt is: <bos> = a <ank>
predicting: =
prompt is: <bos> = a <ank> =
predicting: =
prompt is: <bos> = a <ank> = =
predicting: =
prompt is: <bos> = a <ank> = = =
predicting: =
prompt is: <bos> = a <ank> = = = =
predicting: =
prompt is: <bos> = a <ank> = = = = =
predicting: =
prompt is: <bos> = a <ank> = = = = = =
predicting: =
prompt is: <bos> = a <ank> = = = = = = =
<bos> = a <ank> = = = = = = =
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
| 3/3 [00:00<00:00, 1631.81it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████████████████| 36718/36
718 [00:00<00:00, 134509.68it/s]
number of train vocabs: 33277
number of train tokens: 2051910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████████████████| 36718/36
718 [00:00<00:00, 115609.70it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████████████████| 36718/36
718 [00:00<00:00, 258001.83it/s]
100%|██████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████████| 36718/367
18 [00:00<00:00, 3760198.61it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████████████| 36718/3
6718 [00:01<00:00, 36222.97it/s]
100%|██████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████████████| 3760/
3760 [00:00<00:00, 37484.03it/s]
100%|██████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████████████| 4358/
4358 [00:00<00:00, 37436.56it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 17.0 M
1 | lstm_layer         | LSTM      | 8.4 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 17.1 M
-------------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.913   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.461685180664062     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 14.
79it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 17.0 M │
│ 1 │ lstm_layer         │ LSTM      │  8.4 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 17.1 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 25.5 M


Non-trainable params: 0


Total params: 25.5 M


Total estimated model params size (MB): 101


Epoch 7/99 ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━ 144/371 0:00:28 • 0:00:43 5
Epoch 00050: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00053: reducing learning rate of group 0 to 2.5000e-04.
Epoch 57/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:20 • 0:00:01
5.36it/s v_num: gzps loss: 4.245 val_loss: 4.646
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35505.43it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:30<
00:00, 11.99it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 33523.72it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  7.04it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 39626.04it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  7.05it/s]
Training Perplexity: 40.70444577617088
        Perplexity with OOVs: 102.71584224985155
        Perplexity without OOVs: 102.71584224985155
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4692.69 seconds
predicting: =
prompt is: <bos> =
predicting: =
prompt is: <bos> = =
predicting: =
prompt is: <bos> = = =
predicting: =
prompt is: <bos> = = = =
predicting: <unk>
prompt is: <bos> = = = = <unk>
predicting: =
prompt is: <bos> = = = = <unk> =
predicting: =
prompt is: <bos> = = = = <unk> = =
predicting: =
prompt is: <bos> = = = = <unk> = = =
predicting: =
prompt is: <bos> = = = = <unk> = = = =
predicting: =
prompt is: <bos> = = = = <unk> = = = = =
<bos> = = = = <unk> = = = = =
training on consonants english
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 142820.34it/s]
number of train vocabs: 25538
number of train tokens: 2012005
train_entropy: 9.781745212611602
Some of the Dataset Samples after deleting vowels:

= Vlkyr Chrncls  =

Snjō n Vlkyr 3 : <nk> Chrncls ( Jpns : 戦場のヴァルキュリア3 , lt . Vlkyr f th
Bttlfld 3 ) , cmmnly rfrrd t s Vlkyr Chrncls  tsd Jpn , s  tctcl rl @-@ plyng v
d gm dvlpd by Sg nd Md.Vsn fr th PlySttn Prtbl . Rlsd n Jnry 2011 n Jpn , t s t
h thrd gm n th Vlkyr srs . <nk> th sm fsn f tctcl nd rl @-@ tm gmply s ts prdcs
srs , th stry rns prlll t th frst gm nd fllws th " Nmlss " ,  pnl mltry nt srvn
g th ntn f Gll drng th Scnd rpn Wr wh prfrm scrt blck prtns nd r pttd gnst th m
prl nt " <nk> Rvn " .
Th gm bgn dvlpmnt n 2010 , crryng vr  lrg prtn f th wrk dn n Vlkyr Chrncls  . W
hl t rtnd th stndrd ftrs f th srs , t ls ndrwnt mltpl djstmnts , sch s mkng th
gm mr <nk> fr srs nwcmrs . Chrctr dsgnr <nk> Hnj nd cmpsr Htsh Skmt bth rtrnd f
rm prvs ntrs , lng wth Vlkyr Chrncls  drctr Tksh zw .  lrg tm f wrtrs hndld th
scrpt . Th gm 's pnng thm ws sng by My 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 118440.90it/s]
Considered Vocab (from WordTokenizer): 25,542
        All Vocab (WordTokenizer): 25,542
Training WordTokenizer ...
Tokenizer Vocab Size: 25,542
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 222977.04it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3624107.64it/s]
Sequence Length: 211
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:00<00:
00, 37056.14it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 31722.49it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 38523.16it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 13.1 M
1 | lstm_layer         | LSTM      | 8.4 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 13.1 M
-------------------------------------------------
21.5 M    Trainable params
0         Non-trainable params
21.5 M    Total params
86.032    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.166426658630371     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 18.
17it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 13.1 M │
│ 1 │ lstm_layer         │ LSTM      │  8.4 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 13.1 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 21.5 M



Non-trainable params: 0



Total params: 21.5 M



Total estimated model params size (MB): 86



Epoch 00057: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00059: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00061: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00063: reducing learning rate of group 0 to 6.2500e-05.
Epoch 64/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:06 • 0:00:01
6.67it/s v_num: mksf loss: 4.271 val_loss: 4.678
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35056.81it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:25<
00:00, 14.79it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 34977.75it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:05<
00:00,  7.53it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 39891.96it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:05<
00:00,  7.57it/s]
Training Perplexity: 44.782352553805055
        Perplexity with OOVs: 106.20146642046056
        Perplexity without OOVs: 106.20146642046056
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4323.73 seconds
predicting: =
prompt is: <bos> =
predicting: =
prompt is: <bos> = =
predicting: =
prompt is: <bos> = = =
predicting: =
prompt is: <bos> = = = =
predicting: <nk>
prompt is: <bos> = = = = <nk>
predicting: =
prompt is: <bos> = = = = <nk> =
predicting: =
prompt is: <bos> = = = = <nk> = =
predicting: =
prompt is: <bos> = = = = <nk> = = =
predicting: =
prompt is: <bos> = = = = <nk> = = = =
predicting: =
prompt is: <bos> = = = = <nk> = = = = =
<bos> = = = = <nk> = = = = =
training on masked consonants english
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 134645.63it/s]
number of train vocabs: 30090
number of train tokens: 2051910
train_entropy: 10.021776230734403
Some of the Dataset Samples after masking:

= Valkyraa Chranaclas aaa =

Sanjō na Valkyraa 3 : <ank> Chranaclas ( Japanasa : 戦場のヴァルキュリア3 , lat
 . Valkyraa af tha Battlafaald 3 ) , cammanly rafarrad ta as Valkyraa Chranacla
s aaa aatsada Japan , as a tactacal rala @-@ playang vadaa gama davalapad by Sa
ga and Madaa.Vasaan far tha PlayStataan Partabla . Ralaasad an Janaary 2011 an
Japan , at as tha thard gama an tha Valkyraa saraas . <ank> tha sama fasaan af
tactacal and raal @-@ tama gamaplay as ats pradacassars , tha stary rans parall
al ta tha farst gama and fallaws tha " Namalass " , a panal malatary anat sarva
ng tha nataan af Gallaa darang tha Sacand aarapan War wha parfarm sacrat black
aparataans and ara pattad agaanst tha amparaal anat " <ank> Ravan " .
Tha gama bagan davalapmant an 2010 , carryang avar a larga partaan af tha wark
dana an Valkyraa Chranaclas aa . Whala at rataanad tha standard faataras af tha
 saraas , at alsa andarwant maltapla adjastmants , sach as makang tha gama mara
 <ank> far saraas nawcamars . Charactar dasagnar <ank> Hanjaa and campasar Hata
sha Sakamata bath ratarnad fram pravaaas antraas , alang wath Valkyraa Chranacl
as aa daractar Takasha azawa . a larga taam af wratars handlad tha scrapt . Tha
 gama 's apanang thama was sang by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 109530.27it/s]
Considered Vocab (from WordTokenizer): 30,094
        All Vocab (WordTokenizer): 30,094
Training WordTokenizer ...
Tokenizer Vocab Size: 30,094
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 204265.32it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3837784.50it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 34389.89it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36357.27it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 36918.32it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 15.4 M
1 | lstm_layer         | LSTM      | 8.4 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 15.4 M
-------------------------------------------------
23.8 M    Trainable params
0         Non-trainable params
23.8 M    Total params
95.373    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.343424797058105     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 15.
56it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 15.4 M │
│ 1 │ lstm_layer         │ LSTM      │  8.4 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 15.4 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 23.8 M



Non-trainable params: 0



Total params: 23.8 M



Total estimated model params size (MB): 95



/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.10/site-
packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardIn
terrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Epoch 4/99 ━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96/371 0:00:18 • 0:00:47 5.
85it/s v_num: h3ic loss: 5.787 val_loss: 5.427
^C
Aborted!
^C^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ ^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ ^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1

Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1632.87it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 134434.18it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 115236.60it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 257231.76it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3579713.97it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35517.14it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36530.93it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 31140.80it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
Traceback (most recent call last):
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 204, in <module>
    run()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 112, in run
    training_pipeline(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/training_pipeline.py", line 146, in train
ing_pipeline
    lm_model = LitNeuralLanguageModel(vocab_size=tokenizer.vocab_size)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 66, in __init__
    assert (
AssertionError: in weights tieing, embedding size should be the same as hidden
size
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1633.30it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 133604.40it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 115029.34it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 257872.23it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3715385.74it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35902.67it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 37071.67it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 31488.53it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 17.0 M
1 | lstm_layer         | LSTM      | 13.2 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 17.1 M
-------------------------------------------------
30.3 M    Trainable params
0         Non-trainable params
30.3 M    Total params
121.021   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

Traceback (most recent call last):
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 204, in <module>
    run()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 112, in run
    training_pipeline(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/training_pipeline.py", line 160, in train
ing_pipeline
    trainer = train_lm(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/utils.py", line 148, in train_lm
    trainer.validate(
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/trainer.py", line 609, in validate
    return call._call_and_handle_interrupt(
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_hand
le_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/trainer.py", line 652, in _validate_
impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/trainer.py", line 935, in _run
    results = self._run_stage()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/trainer.py", line 971, in _run_stage
    return self._evaluation_loop.run()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/loops/utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/loops/evaluation_loop.py", line 375, in _eva
luation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values()
)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/trainer/call.py", line 288, in _call_strateg
y_hook
    output = fn(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/pytorch_lightning/strategies/strategy.py", line 378, in valida
tion_step
    return self.model.validation_step(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 126, in validation_step
    loss = self._get_loss(batch)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 89, in _get_loss
    outputs, hiddens = self(inputs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 84, in forward
    outputs = self.second_dense_layer(outputs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (13760x650 and 512x3328
1)
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1618.80it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 133939.15it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 115318.92it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 257495.83it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3705463.03it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35642.63it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36614.47it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 31317.56it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 21.6 M
1 | lstm_layer         | LSTM      | 13.5 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 21.7 M
-------------------------------------------------
35.2 M    Trainable params
0         Non-trainable params
35.2 M    Total params
140.827   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.428852081298828     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:03 • 0:00:00 10.
91it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 21.6 M │
│ 1 │ lstm_layer         │ LSTM      │ 13.5 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 21.7 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 35.2 M



Non-trainable params: 0



Total params: 35.2 M



Total estimated model params size (MB): 140



Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finaliz
ers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp
_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 730, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 728, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-q0ecbqng'
Epoch 00048: reducing learning rate of group 0 to 5.0000e-04.
/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.10/site-
packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardIn
terrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Epoch 48/99 ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 130/371 0:00:39 • 0:01:11
3.43it/s v_num: 55hn loss: 4.055 val_loss: 4.645
^C
Aborted!
^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1623.39it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 133268.08it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 114775.76it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 255517.37it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3658371.24it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 33942.50it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 34625.84it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 34592.23it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
Traceback (most recent call last):
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 204, in <module>
    run()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 112, in run
    training_pipeline(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/training_pipeline.py", line 146, in train
ing_pipeline
    lm_model = LitNeuralLanguageModel(vocab_size=tokenizer.vocab_size)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 66, in __init__
    assert (
AssertionError: in weights tieing, embedding size should be the same as hidden
size
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1611.75it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 132897.54it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 114297.12it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 256088.42it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3747024.51it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35625.29it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36784.93it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 36624.06it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name               | Type      | Params
-------------------------------------------------
0 | embedding_layer    | Embedding | 17.0 M
1 | lstm_layer         | LSTM      | 8.4 M
2 | dropout_layer      | Dropout   | 0
3 | relu               | ReLU      | 0
4 | second_dense_layer | Linear    | 17.1 M
-------------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.913   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.461685180664062     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 14.
57it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name               ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer    │ Embedding │ 17.0 M │
│ 1 │ lstm_layer         │ LSTM      │  8.4 M │
│ 2 │ dropout_layer      │ Dropout   │      0 │
│ 3 │ relu               │ ReLU      │      0 │
│ 4 │ second_dense_layer │ Linear    │ 17.1 M │
└───┴────────────────────┴───────────┴────────┘
Trainable params: 25.5 M



Non-trainable params: 0



Total params: 25.5 M



Total estimated model params size (MB): 101



Epoch 00050: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00053: reducing learning rate of group 0 to 2.5000e-04.
Epoch 57/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:20 • 0:00:01
5.36it/s v_num: qwv9 loss: 4.245 val_loss: 4.646
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 27617.52it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:30<
00:00, 12.11it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 32563.80it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  6.95it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 39186.83it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  6.96it/s]
Training Perplexity: 40.70444577617088
        Perplexity with OOVs: 102.71584224985155
        Perplexity without OOVs: 102.71584224985155
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4702.82 seconds
predicting: =
prompt is: <bos> =
Traceback (most recent call last):
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 204, in <module>
    run()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/english_consonants
/experiments/language_modelling/run_experiment.py", line 112, in run
    training_pipeline(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/training_pipeline.py", line 234, in train
ing_pipeline
    generate_text(
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/utils.py", line 49, in generate_text
    output, hiddens = lm_model(encoded, hiddens)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/EnglishConsonants/./english_consonan
ts/experiments/language_modelling/src/models.py", line 78, in forward
    outputs, hiddens = self.gru_layer(outputs, hiddens)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'LitNeuralLanguageModel' object has no attribute 'gru_layer'
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[
B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[
[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^
[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[
B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[
[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[BFound cached dataset wikitext (/home/maje
d_alshaibani/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db529
02eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1633.08it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 134585.85it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
^C
Aborted!
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/__main__.py", line 3, in <module>
    cli.cli(prog_name="python -m wandb")
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/cli/cli.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/cli/cli.py", line 289, in service
    server.serve()
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/sdk/service/server.py", line 139, in serve
    self._inform_used_ports(grpc_port=grpc_port, sock_port=sock_port)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/sdk/service/server.py", line 67, in _inform_used_ports
    pf.write(self._port_fname)
  File "/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.
10/site-packages/wandb/sdk/service/port_file.py", line 26, in write
    f = tempfile.NamedTemporaryFile(prefix=bname, dir=dname, mode="w", delete=F
alse)
  File "/usr/lib/python3.10/tempfile.py", line 698, in NamedTemporaryFile
    file = _io.open(dir, mode, buffering=buffering,
  File "/usr/lib/python3.10/tempfile.py", line 695, in opener
    fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)
  File "/usr/lib/python3.10/tempfile.py", line 395, in _mkstemp_inner
    fd = _os.open(file, flags, 0o600)
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpvtrolsl8/port-
3353229.txt0e7fbou2'
^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1628.86it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 134981.61it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 117336.11it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 254366.92it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3756621.48it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35693.52it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36895.69it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 36903.56it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type      | Params
----------------------------------------------
0 | embedding_layer | Embedding | 17.0 M
1 | rnn             | LSTM      | 8.4 M
2 | dropout_layer   | Dropout   | 0
3 | relu            | ReLU      | 0
4 | dense_layer     | Linear    | 17.1 M
----------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.913   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.461685180664062     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 14.
84it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding │ 17.0 M │
│ 1 │ rnn             │ LSTM      │  8.4 M │
│ 2 │ dropout_layer   │ Dropout   │      0 │
│ 3 │ relu            │ ReLU      │      0 │
│ 4 │ dense_layer     │ Linear    │ 17.1 M │
└───┴─────────────────┴───────────┴────────┘
Trainable params: 25.5 M



Non-trainable params: 0



Total params: 25.5 M



Total estimated model params size (MB): 101



/home/majed_alshaibani/Experiments/english-consonants-venv/lib/python3.10/site-
packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardIn
terrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Epoch 32/99 ━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 207/371 0:00:45 • 0:00:37
4.48it/s v_num: mipb loss: 4.686 val_loss: 4.714
 20%|█████████████████████████████████████████▎

                                                        | 7262/36718 [00:00<00:
00, 36381.92it/s] 25%|███████████████████████████████████████████████████▍

                                                                         | 9032
/36718 [00:00<00:00, 35971.98it/s]

Aborted!
^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ ^C
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$ python english_consonants/experiments/language_modelling/run_experiment.py
 --gpu_devices=1 --vocab_coverage=1
Found cached dataset wikitext (/home/majed_alshaibani/.cache/huggingface/datase
ts/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13
bcbfa3f8ab646a126)
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 3/3 [00:00<00
:00, 1640.32it/s]
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-1fdf7473581b0bfb.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-fe82178e12600f83.arrow
Loading cached processed dataset at /home/majed_alshaibani/.cache/huggingface/d
atasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499
ceb13bcbfa3f8ab646a126/cache-61ece7f799b57ae9.arrow
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 132980.28it/s]
number of train vocabs: 33,277
number of train tokens: 2,051,910
train_entropy: 10.241532170266325
Some of the Dataset Samples before training:

= Valkyria Chronicles III =

Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit
 . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicle
s III outside Japan , is a tactical role @-@ playing video game developed by Se
ga and Media.Vision for the PlayStation Portable . Released in January 2011 in
Japan , it is the third game in the Valkyria series . <unk> the same fusion of
tactical and real @-@ time gameplay as its predecessors , the story runs parall
el to the first game and follows the " Nameless " , a penal military unit servi
ng the nation of Gallia during the Second Europan War who perform secret black
operations and are pitted against the Imperial unit " <unk> Raven " .
The game began development in 2010 , carrying over a large portion of the work
done on Valkyria Chronicles II . While it retained the standard features of the
 series , it also underwent multiple adjustments , such as making the game more
 <unk> for series newcomers . Character designer <unk> Honjou and composer Hito
shi Sakimoto both returned from previous entries , along with Valkyria Chronicl
es II director Takeshi Ozawa . A large team of writers handled the script . The
 game 's opening theme was sung by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 114513.75it/s]
Considered Vocab (from WordTokenizer): 33,281
        All Vocab (WordTokenizer): 33,281
Training WordTokenizer ...
Tokenizer Vocab Size: 33,281
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 257108.51it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3733399.29it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 35663.90it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 36876.80it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 36697.15it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type      | Params
----------------------------------------------
0 | embedding_layer | Embedding | 17.0 M
1 | rnn             | LSTM      | 8.4 M
2 | dropout_layer   | Dropout   | 0
3 | relu            | ReLU      | 0
4 | dense_layer     | Linear    | 17.1 M
----------------------------------------------
25.5 M    Trainable params
0         Non-trainable params
25.5 M    Total params
101.913   Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.461685180664062     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 14.
58it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding │ 17.0 M │
│ 1 │ rnn             │ LSTM      │  8.4 M │
│ 2 │ dropout_layer   │ Dropout   │      0 │
│ 3 │ relu            │ ReLU      │      0 │
│ 4 │ dense_layer     │ Linear    │ 17.1 M │
└───┴─────────────────┴───────────┴────────┘
Trainable params: 25.5 M



Non-trainable params: 0



Total params: 25.5 M



Total estimated model params size (MB): 101



Traceback (most recent call last):
  File "/usr/lib/python3.10/multiprocessing/util.py", line 300, in _run_finaliz
ers
    finalizer()
  File "/usr/lib/python3.10/multiprocessing/util.py", line 224, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/usr/lib/python3.10/multiprocessing/util.py", line 133, in _remove_temp
_dir
    rmtree(tempdir)
  File "/usr/lib/python3.10/shutil.py", line 730, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File "/usr/lib/python3.10/shutil.py", line 728, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/tmp/pymp-6eh8hwf9'
Epoch 00054: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00058: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00062: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00064: reducing learning rate of group 0 to 6.2500e-05.
Epoch 65/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:20 • 0:00:01
5.36it/s v_num: z0cw loss: 4.026 val_loss: 4.641
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 34516.40it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:30<
00:00, 12.17it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 33770.48it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  7.08it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 38436.08it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  7.07it/s]
Training Perplexity: 37.61623228588132
        Perplexity with OOVs: 102.46624255099495
        Perplexity without OOVs: 102.46624255099495
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 5339.01 seconds
predicting: The
prompt is: <bos> The
predicting: <unk>
prompt is: <bos> The <unk>
predicting: and
prompt is: <bos> The <unk> and
predicting: <unk>
prompt is: <bos> The <unk> and <unk>
predicting: of
prompt is: <bos> The <unk> and <unk> of
predicting: the
prompt is: <bos> The <unk> and <unk> of the
predicting: <unk>
prompt is: <bos> The <unk> and <unk> of the <unk>
predicting: is
prompt is: <bos> The <unk> and <unk> of the <unk> is
predicting: <unk>
prompt is: <bos> The <unk> and <unk> of the <unk> is <unk>
predicting: ,
prompt is: <bos> The <unk> and <unk> of the <unk> is <unk> ,
<bos> The <unk> and <unk> of the <unk> is <unk> ,
training on consonants english
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 138999.39it/s]
number of train vocabs: 25538
number of train tokens: 2012005
train_entropy: 9.781745212611602
Some of the Dataset Samples after deleting vowels:

= Vlkyr Chrncls  =

Snjō n Vlkyr 3 : <nk> Chrncls ( Jpns : 戦場のヴァルキュリア3 , lt . Vlkyr f th
Bttlfld 3 ) , cmmnly rfrrd t s Vlkyr Chrncls  tsd Jpn , s  tctcl rl @-@ plyng v
d gm dvlpd by Sg nd Md.Vsn fr th PlySttn Prtbl . Rlsd n Jnry 2011 n Jpn , t s t
h thrd gm n th Vlkyr srs . <nk> th sm fsn f tctcl nd rl @-@ tm gmply s ts prdcs
srs , th stry rns prlll t th frst gm nd fllws th " Nmlss " ,  pnl mltry nt srvn
g th ntn f Gll drng th Scnd rpn Wr wh prfrm scrt blck prtns nd r pttd gnst th m
prl nt " <nk> Rvn " .
Th gm bgn dvlpmnt n 2010 , crryng vr  lrg prtn f th wrk dn n Vlkyr Chrncls  . W
hl t rtnd th stndrd ftrs f th srs , t ls ndrwnt mltpl djstmnts , sch s mkng th
gm mr <nk> fr srs nwcmrs . Chrctr dsgnr <nk> Hnj nd cmpsr Htsh Skmt bth rtrnd f
rm prvs ntrs , lng wth Vlkyr Chrncls  drctr Tksh zw .  lrg tm f wrtrs hndld th
scrpt . Th gm 's pnng thm ws sng by My 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 116654.68it/s]
Considered Vocab (from WordTokenizer): 25,542
        All Vocab (WordTokenizer): 25,542
Training WordTokenizer ...
Tokenizer Vocab Size: 25,542
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 225466.48it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3568681.60it/s]
Sequence Length: 211
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 36534.35it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 31525.97it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 37713.24it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type      | Params
----------------------------------------------
0 | embedding_layer | Embedding | 13.1 M
1 | rnn             | LSTM      | 8.4 M
2 | dropout_layer   | Dropout   | 0
3 | relu            | ReLU      | 0
4 | dense_layer     | Linear    | 13.1 M
----------------------------------------------
21.5 M    Trainable params
0         Non-trainable params
21.5 M    Total params
86.032    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.166426658630371     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 18.
15it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding │ 13.1 M │
│ 1 │ rnn             │ LSTM      │  8.4 M │
│ 2 │ dropout_layer   │ Dropout   │      0 │
│ 3 │ relu            │ ReLU      │      0 │
│ 4 │ dense_layer     │ Linear    │ 13.1 M │
└───┴─────────────────┴───────────┴────────┘
Trainable params: 21.5 M



Non-trainable params: 0



Total params: 21.5 M



Total estimated model params size (MB): 86



Epoch 00051: reducing learning rate of group 0 to 5.0000e-04.
Epoch 56/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:05 • 0:00:01
6.74it/s v_num: bke4 loss: 4.303 val_loss: 4.682
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 34918.92it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:25<
00:00, 14.82it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 33139.96it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:05<
00:00,  7.54it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 39019.86it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:05<
00:00,  7.58it/s]
Training Perplexity: 47.11253877686215
        Perplexity with OOVs: 106.8343163903084
        Perplexity without OOVs: 106.8343163903084
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 3783.64 seconds
predicting: n
prompt is: <bos> n
predicting: Sptmbr
prompt is: <bos> n Sptmbr
predicting: 2010
prompt is: <bos> n Sptmbr 2010
predicting: ,
prompt is: <bos> n Sptmbr 2010 ,
predicting: th
prompt is: <bos> n Sptmbr 2010 , th
predicting: R
prompt is: <bos> n Sptmbr 2010 , th R
predicting: &
prompt is: <bos> n Sptmbr 2010 , th R &
predicting: B
prompt is: <bos> n Sptmbr 2010 , th R & B
predicting: /
prompt is: <bos> n Sptmbr 2010 , th R & B /
predicting: Hp
prompt is: <bos> n Sptmbr 2010 , th R & B / Hp
<bos> n Sptmbr 2010 , th R & B / Hp
training on masked consonants english
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 130460.54it/s]
number of train vocabs: 30090
number of train tokens: 2051910
train_entropy: 10.021776230734403
Some of the Dataset Samples after masking:

= Valkyraa Chranaclas aaa =

Sanjō na Valkyraa 3 : <ank> Chranaclas ( Japanasa : 戦場のヴァルキュリア3 , lat
 . Valkyraa af tha Battlafaald 3 ) , cammanly rafarrad ta as Valkyraa Chranacla
s aaa aatsada Japan , as a tactacal rala @-@ playang vadaa gama davalapad by Sa
ga and Madaa.Vasaan far tha PlayStataan Partabla . Ralaasad an Janaary 2011 an
Japan , at as tha thard gama an tha Valkyraa saraas . <ank> tha sama fasaan af
tactacal and raal @-@ tama gamaplay as ats pradacassars , tha stary rans parall
al ta tha farst gama and fallaws tha " Namalass " , a panal malatary anat sarva
ng tha nataan af Gallaa darang tha Sacand aarapan War wha parfarm sacrat black
aparataans and ara pattad agaanst tha amparaal anat " <ank> Ravan " .
Tha gama bagan davalapmant an 2010 , carryang avar a larga partaan af tha wark
dana an Valkyraa Chranaclas aa . Whala at rataanad tha standard faataras af tha
 saraas , at alsa andarwant maltapla adjastmants , sach as makang tha gama mara
 <ank> far saraas nawcamars . Charactar dasagnar <ank> Hanjaa and campasar Hata
sha Sakamata bath ratarnad fram pravaaas antraas , alang wath Valkyraa Chranacl
as aa daractar Takasha azawa . a larga taam af wratars handlad tha scrapt . Tha
 gama 's apanang thama was sang by May 'n .
Global seed set to 42
Train Samples: 36,718
        Val Samples: 3,760
        Test Samples: 4,358
Calculating vocab size using WordTokenizer:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 106758.84it/s]
Considered Vocab (from WordTokenizer): 30,094
        All Vocab (WordTokenizer): 30,094
Training WordTokenizer ...
Tokenizer Vocab Size: 30,094
Calculating Sequence Length:
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████| 36718/36718 [00:00<00:0
0, 214363.50it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████| 36718/36718 [00:00<00:00
, 3494666.42it/s]
Sequence Length: 216
Building DataLoaders
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 34063.21it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 3760/3760 [00:00<00:
00, 30362.79it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 35875.00it/s]
Train DataLoader: 371
        Val DataLoader: 38
        Test DataLoader: 45
| Name            | Type      | Params
----------------------------------------------
0 | embedding_layer | Embedding | 15.4 M
1 | rnn             | LSTM      | 8.4 M
2 | dropout_layer   | Dropout   | 0
3 | relu            | ReLU      | 0
4 | dense_layer     | Linear    | 15.4 M
----------------------------------------------
23.8 M    Trainable params
0         Non-trainable params
23.8 M    Total params
95.373    Total estimated model params size (MB)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃      Validate metric      ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│         val_loss          │    10.343424797058105     │
└───────────────────────────┴───────────────────────────┘
Validation ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 0:00:02 • 0:00:00 15.
58it/s
You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To prop
erly utilize them, you should set `torch.set_float32_matmul_precision('medium'
| 'high')` which will trade-off precision for performance. For more details, re
ad https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision
.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
┏━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name            ┃ Type      ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ embedding_layer │ Embedding │ 15.4 M │
│ 1 │ rnn             │ LSTM      │  8.4 M │
│ 2 │ dropout_layer   │ Dropout   │      0 │
│ 3 │ relu            │ ReLU      │      0 │
│ 4 │ dense_layer     │ Linear    │ 15.4 M │
└───┴─────────────────┴───────────┴────────┘
Trainable params: 23.8 M



Non-trainable params: 0



Total params: 23.8 M



Total estimated model params size (MB): 95



Epoch 00054: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00057: reducing learning rate of group 0 to 2.5000e-04.
Epoch 59/99 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 370/371 0:01:15 • 0:00:01
5.89it/s v_num: d3dy loss: 4.204 val_loss: 4.618
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████| 36718/36718 [00:01<00:
00, 21404.69it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
██████████████████████████████████████████████████████████████| 371/371 [00:29<
00:00, 12.79it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 32521.85it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  6.69it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
█████████████████████████████████████████████████████████| 4358/4358 [00:00<00:
00, 36717.42it/s]
100%|██████████████████████████████████████████████████████████████████████████
███████████████████████████████████████████████████████████████████████████████
████████████████████████████████████████████████████████████████| 45/45 [00:06<
00:00,  6.69it/s]
Training Perplexity: 39.46116501776771
        Perplexity with OOVs: 100.28801044942578
        Perplexity without OOVs: 100.28801044942578
Training OOVs rate: 0.00
        Validation OOVs rate: 0.00
        Test OOVs rate: 0.00
Training Time: 4547.98 seconds
predicting: =
prompt is: <bos> =
predicting: =
prompt is: <bos> = =
predicting: =
prompt is: <bos> = = =
predicting: <ank>
prompt is: <bos> = = = <ank>
predicting: =
prompt is: <bos> = = = <ank> =
predicting: =
prompt is: <bos> = = = <ank> = =
predicting: =
prompt is: <bos> = = = <ank> = = =
predicting: =
prompt is: <bos> = = = <ank> = = = =
predicting: =
prompt is: <bos> = = = <ank> = = = = =
predicting: =
prompt is: <bos> = = = <ank> = = = = = =
<bos> = = = <ank> = = = = = =
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishC
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConson
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
(english-consonants-venv) majed_alshaibani@jrcai14:~/Experiments/EnglishConsona
nts$
